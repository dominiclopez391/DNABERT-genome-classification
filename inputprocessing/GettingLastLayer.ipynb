{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14030497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, DNATokenizer, BertConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea44cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_finetune import MODEL_CLASSES, ALL_MODELS, processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0649a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required parameters\n",
    "parser.add_argument(\n",
    "    \"--data_dir\",\n",
    "    default='/home/015861469/DNABERT-genome-classification/examples/sample_data/ft/6-2',\n",
    "    type=str,\n",
    "    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_type\",\n",
    "    default='dna',\n",
    "    type=str,\n",
    "    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_process\",\n",
    "    default=2,\n",
    "    type=int,\n",
    "    help=\"number of processes used for data process\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--should_continue\", action=\"store_true\", help=\"Whether to continue from latest checkpoint in output_dir\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default='/home/015861469/DNABERT-genome-classification/examples/finetuned-model',\n",
    "    type=str,\n",
    "    help=\"Path to pre-trained model or shortcut name selected in the list: \" + \", \".join(ALL_MODELS),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--task_name\",\n",
    "    default='dna-genome-classification',\n",
    "    type=str,\n",
    "    help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_dir\",\n",
    "    default='/home/015861469/dnabert-finetune2/output',\n",
    "    type=str,\n",
    "    help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    ")\n",
    "\n",
    "\n",
    "# Other parameters\n",
    "parser.add_argument(\n",
    "    \"--visualize_data_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--result_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The directory where the dna690 and mouse will save results.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_name\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\",\n",
    "    default=\"\",\n",
    "    type=str,\n",
    "    help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--predict_dir\",\n",
    "    default=None,\n",
    "    type=str,\n",
    "    help=\"The output directory of predicted result. (when do_predict)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_seq_length\",\n",
    "    default=150,\n",
    "    type=int,\n",
    "    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "    \"than this will be truncated, sequences shorter will be padded.\",\n",
    ")\n",
    "parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
    "parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
    "parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to do prediction on the given dataset.\")\n",
    "parser.add_argument(\"--do_visualize\", action=\"store_true\", help=\"Whether to calculate attention score.\")\n",
    "parser.add_argument(\"--visualize_train\", action=\"store_true\", help=\"Whether to visualize train.tsv or dev.tsv.\")\n",
    "parser.add_argument(\"--do_ensemble_pred\", action=\"store_true\", help=\"Whether to do ensemble prediction with kmer 3456.\")\n",
    "parser.add_argument(\n",
    "    \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--do_lower_case\", action=\"store_true\", help=\"Set this flag if you are using an uncased model.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_train_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for training.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_eval_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for evaluation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--per_gpu_pred_batch_size\", default=8, type=int, help=\"Batch size per GPU/CPU for prediction.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--early_stop\", default=0, type=int, help=\"set this to a positive integet if you want to perfrom early stop. The model will stop \\\n",
    "                                                if the auc keep decreasing early_stop times\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--predict_scan_size\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gradient_accumulation_steps\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    ")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--beta1\", default=0.9, type=float, help=\"Beta1 for Adam optimizer.\")\n",
    "parser.add_argument(\"--beta2\", default=0.999, type=float, help=\"Beta2 for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--attention_probs_dropout_prob\", default=0.1, type=float, help=\"Dropout rate of attention.\")\n",
    "parser.add_argument(\"--hidden_dropout_prob\", default=0.1, type=float, help=\"Dropout rate of intermidiete layer.\")\n",
    "parser.add_argument(\"--rnn_dropout\", default=0.0, type=float, help=\"Dropout rate of intermidiete layer.\")\n",
    "parser.add_argument(\"--rnn\", default=\"lstm\", type=str, help=\"What kind of RNN to use\")\n",
    "parser.add_argument(\"--num_rnn_layer\", default=2, type=int, help=\"Number of rnn layers in dnalong model.\")\n",
    "parser.add_argument(\"--rnn_hidden\", default=768, type=int, help=\"Number of hidden unit in a rnn layer.\")\n",
    "parser.add_argument(\n",
    "    \"--num_train_epochs\", default=3.0, type=float, help=\"Total number of training epochs to perform.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_steps\",\n",
    "    default=-1,\n",
    "    type=int,\n",
    "    help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
    ")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\"--warmup_percent\", default=0, type=float, help=\"Linear warmup over warmup_percent*total_steps.\")\n",
    "\n",
    "parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
    "parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
    "parser.add_argument(\n",
    "    \"--save_total_limit\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--eval_all_checkpoints\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\",\n",
    ")\n",
    "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_models\", type=int, default=None, help=\"The model used to do visualization. If None, use 3456.\",\n",
    ")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
    "\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--fp16\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--fp16_opt_level\",\n",
    "    type=str,\n",
    "    default=\"O1\",\n",
    "    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "    \"See details at https://nvidia.github.io/apex/amp.html\",\n",
    ")\n",
    "parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f377ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\", 0)\n",
    "\n",
    "def device_mem():\n",
    "    free_memory, total_memory = torch.cuda.mem_get_info(device)\n",
    "    \n",
    "    print(f'mem allocated: {(total_memory - free_memory)/(1073741824)} GB')\n",
    "    print(f'mem free: {free_memory/(1073741824)} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "466dfa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "<class 'transformers.tokenization_dna.DNATokenizer'>\n",
      "mem allocated: 0.8946533203125 GB\n",
      "mem free: 11.01788330078125 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('checkpoint-176000')\n",
    "model = BertForSequenceClassification.from_pretrained('checkpoint-176000', output_hidden_states=True)\n",
    "tokenizer = DNATokenizer.from_pretrained('checkpoint-176000', do_lower_case=args.do_lower_case)\n",
    "\n",
    "device_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d954bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_finetune import load_and_cache_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7225fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c7e009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_finetune import SequentialSampler, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f087fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred_sampler = SequentialSampler(test_dataset)\n",
    "pred_dataloader = DataLoader(test_dataset, sampler=pred_sampler, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d4d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_input = tokenizer.encode([\"AAAAAA\", \"CGGGCC\", \"CGGGCC\", \"CGGGCC\"], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d40408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mem allocated: 0.8946533203125 GB\n",
      "mem free: 11.01788330078125 GB\n",
      "mem allocated: 1.2813720703125 GB\n",
      "mem free: 10.63116455078125 GB\n"
     ]
    }
   ],
   "source": [
    "device_mem()\n",
    "\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "device_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d659f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 20194/66666 [03:03<06:58, 110.96it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "preds = []\n",
    "\n",
    "num_batches = 0\n",
    "\n",
    "for batch in tqdm(pred_dataloader):\n",
    "    # device_mem()\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=batch[0], attention_mask=batch[1])\n",
    "        ground_truth = batch[3].detach().cpu().numpy()\n",
    "        predictions = output[0].detach().cpu().numpy()\n",
    "        #hidden_state = output[1][-1].detach().cpu().numpy()\n",
    "        \n",
    "        '''\n",
    "        for hidden_tensor in output[1]:\n",
    "            \n",
    "            \n",
    "            \n",
    "            if hidden_state is None:\n",
    "                hidden_state = hidden_tensor.detach().cpu().numpy()\n",
    "            else:\n",
    "                hidden_state = np.vstack((hidden_state, hidden_tensor.detach().cpu().numpy()))\n",
    "                \n",
    "        '''\n",
    "\n",
    "        #preds.append([ground_truth, predictions, hidden_state])\n",
    "        preds.append([ground_truth, predictions])\n",
    "    batch = tuple(t.to('cpu') for t in batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9ffaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7932"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# np.argmax(preds[0][0])\n",
    "len(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3822f210",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(preds, \"allsampleswithlasthiddenlayer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdff687d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([10]),\n",
       " array([[-1.3957361 , -1.4652294 , -0.5089347 , -1.3103321 , -2.1526408 ,\n",
       "         -2.4359052 , -0.59401613, -1.7563827 ,  0.18181531, -1.847075  ,\n",
       "          0.10288739, -1.6903058 , -1.1824719 ,  0.83473694, -0.845769  ,\n",
       "         -1.716885  , -0.39154914,  1.0610424 ,  0.41459253, -2.7450264 ,\n",
       "         -1.6299285 ,  0.36574784, -1.9848994 , -1.8938447 ,  9.199543  ,\n",
       "          0.62960726]], dtype=float32),\n",
       " tensor([[[-0.6737,  1.8087, -1.0561,  ..., -1.5085, -0.6884,  0.1325],\n",
       "          [-2.0218,  0.1378,  0.0759,  ..., -1.6328,  0.1259, -1.3913],\n",
       "          [-1.5478, -0.1952,  0.5750,  ..., -0.4590,  1.3949,  0.4827],\n",
       "          ...,\n",
       "          [ 0.8589,  0.9341, -1.8443,  ..., -0.3697,  0.1014,  0.4760],\n",
       "          [ 0.8588,  0.9342, -1.8443,  ..., -0.3696,  0.1015,  0.4761],\n",
       "          [ 0.8589,  0.9341, -1.8443,  ..., -0.3697,  0.1015,  0.4761]]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_dataset)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ae1b289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "928e3e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8fa8a456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.9895, -1.3488, -6.2393, -6.4248, -4.7668, -5.7695,  9.8707, -1.6136,\n",
       "           0.6870, -1.3251, -3.2916, -4.1163, -3.1736,  1.0745, -3.7624, -3.9735,\n",
       "          -1.6632, -4.4030, -4.4965, -4.8065, -3.5700, -3.2713, -3.9811, -4.3459,\n",
       "          -2.2340, -2.1067]]),\n",
       " (tensor([[[-0.1898, -2.4788, -1.2954,  ..., -1.0075,  0.4486, -0.3974],\n",
       "           [-0.4983, -0.4832,  0.4338,  ..., -0.1776, -1.0141,  1.0393],\n",
       "           [-1.2430, -0.0303, -1.8768,  ...,  0.1568, -1.0915, -0.1246],\n",
       "           [-0.0042, -0.9437, -2.1916,  ...,  0.2222, -1.6541, -0.1496],\n",
       "           [-0.9131, -0.3854, -0.8939,  ...,  0.2014, -0.7817, -0.8353],\n",
       "           [-0.3086, -0.1756, -0.7787,  ...,  0.5481,  0.8922, -1.9683]]]),\n",
       "  tensor([[[-0.1679, -2.2226,  0.2550,  ..., -0.6580, -0.8625, -0.1905],\n",
       "           [-1.0089, -1.0270,  0.1991,  ..., -0.8095, -0.0135,  1.5513],\n",
       "           [-0.7402, -1.4995, -0.4611,  ..., -0.9995,  0.2757,  1.4518],\n",
       "           [-0.1376, -1.9133, -0.5152,  ..., -1.0038, -0.0444,  1.2699],\n",
       "           [-0.6463, -1.6009,  0.0979,  ..., -0.8398,  0.4032,  1.0659],\n",
       "           [-0.4990,  0.3263, -0.3780,  ...,  1.5655, -0.6043, -1.1951]]]),\n",
       "  tensor([[[ 1.0348, -3.3820, -0.7620,  ..., -0.4593, -0.6379,  0.4461],\n",
       "           [-0.9846, -1.0911, -0.3799,  ..., -0.6063,  0.3863,  0.8147],\n",
       "           [-0.3941, -1.6277, -0.6911,  ..., -0.5877,  0.5949,  0.6308],\n",
       "           [-0.1823, -1.7955, -0.7927,  ..., -0.5763,  0.4991,  0.5618],\n",
       "           [-0.2993, -1.7090, -0.4640,  ..., -0.5088,  0.6611,  0.4459],\n",
       "           [-0.0518, -0.8288, -0.3305,  ...,  0.6993, -1.0178, -0.7497]]]),\n",
       "  tensor([[[ 0.8007, -2.2633, -1.1562,  ...,  0.2159, -0.3483,  0.4083],\n",
       "           [-1.1344, -0.9490, -0.0962,  ..., -0.6106,  0.8576, -0.0119],\n",
       "           [-0.4930, -1.5700, -0.3996,  ..., -0.7396,  0.9757, -0.3404],\n",
       "           [-0.3661, -1.6654, -0.5002,  ..., -0.7551,  0.9075, -0.3792],\n",
       "           [-0.4665, -1.5922, -0.2647,  ..., -0.6569,  0.9784, -0.4448],\n",
       "           [-0.0339, -0.7168, -0.8417,  ...,  0.8294, -0.1636, -0.7941]]]),\n",
       "  tensor([[[ 0.0497, -1.3973,  0.1419,  ..., -0.9681, -0.4752, -0.4280],\n",
       "           [-1.0954, -0.9498,  0.3107,  ..., -0.1708,  1.3689, -0.1397],\n",
       "           [-0.2056, -1.5346, -0.0063,  ..., -0.7076,  1.3795, -0.5212],\n",
       "           [-0.0623, -1.6071, -0.0682,  ..., -0.7189,  1.3240, -0.5842],\n",
       "           [-0.1319, -1.5465,  0.0874,  ..., -0.6727,  1.3934, -0.6126],\n",
       "           [ 0.8670, -1.5496, -0.5424,  ..., -0.3435, -0.1921, -0.5086]]]),\n",
       "  tensor([[[ 0.0156, -0.9801, -0.3201,  ..., -0.3889, -0.4492, -0.2039],\n",
       "           [-1.5058, -1.1856,  0.9672,  ...,  0.6844,  1.1646, -1.2621],\n",
       "           [-0.6022, -1.6832,  0.8895,  ..., -0.2796,  1.1042, -1.3284],\n",
       "           [-0.4865, -1.7285,  0.8383,  ..., -0.2772,  1.0244, -1.4006],\n",
       "           [-0.5412, -1.7260,  0.9233,  ..., -0.2715,  1.1156, -1.3721],\n",
       "           [ 0.4488, -1.0918, -0.7313,  ...,  0.3039, -0.2738, -0.3500]]]),\n",
       "  tensor([[[ 0.2196, -0.2444, -0.2700,  ..., -0.5946, -0.2802, -0.0273],\n",
       "           [-1.4343, -0.7047,  1.1933,  ...,  0.3259,  1.3220, -0.8255],\n",
       "           [-0.5141, -1.0837,  1.2052,  ..., -0.6843,  1.4262, -1.0904],\n",
       "           [-0.4064, -1.1268,  1.1247,  ..., -0.6750,  1.3471, -1.1758],\n",
       "           [-0.4369, -1.1284,  1.2139,  ..., -0.6567,  1.4268, -1.1570],\n",
       "           [ 0.5578, -0.2864, -0.5520,  ..., -0.0664, -0.0784, -0.3300]]]),\n",
       "  tensor([[[-0.1286, -0.2614, -0.1191,  ..., -0.4984, -0.6581, -0.3771],\n",
       "           [-1.9387, -0.5718,  1.6228,  ..., -0.3403,  0.4682, -1.5301],\n",
       "           [-1.1790, -0.9492,  1.4362,  ..., -1.2238,  0.5462, -1.7216],\n",
       "           [-1.0892, -0.9800,  1.3503,  ..., -1.2212,  0.4764, -1.7973],\n",
       "           [-1.1041, -0.9917,  1.4153,  ..., -1.2091,  0.5635, -1.7753],\n",
       "           [ 0.1382, -0.3565, -0.2768,  ..., -0.1197, -0.5091, -0.6490]]]),\n",
       "  tensor([[[-0.3039, -0.3727, -0.8072,  ..., -0.4265, -0.7723, -0.1956],\n",
       "           [-1.9256, -0.4555,  1.2254,  ..., -0.4927,  0.0195, -1.3090],\n",
       "           [-1.2372, -0.9452,  1.0859,  ..., -1.3791, -0.0737, -1.4621],\n",
       "           [-1.1468, -0.9696,  1.0138,  ..., -1.3899, -0.1432, -1.5334],\n",
       "           [-1.1774, -0.9929,  1.0750,  ..., -1.3819, -0.0551, -1.5090],\n",
       "           [-0.0530, -0.4332, -0.9602,  ..., -0.0380, -0.6410, -0.3773]]]),\n",
       "  tensor([[[ 0.0753, -0.6922, -0.5389,  ..., -0.2982, -1.2702, -0.1655],\n",
       "           [-1.6535, -0.5767,  0.7673,  ..., -0.3941, -0.3034, -1.4347],\n",
       "           [-1.0218, -1.0094,  0.6458,  ..., -1.2806, -0.4758, -1.5752],\n",
       "           [-0.9380, -1.0179,  0.5903,  ..., -1.2886, -0.5457, -1.6451],\n",
       "           [-0.9627, -1.0636,  0.6382,  ..., -1.2855, -0.4661, -1.6281],\n",
       "           [ 0.3147, -0.7176, -0.6994,  ..., -0.0365, -1.1797, -0.3167]]]),\n",
       "  tensor([[[ 0.2728, -0.2574, -0.5683,  ...,  0.2468, -1.2238,  0.1927],\n",
       "           [-1.0242, -0.0054,  0.7875,  ...,  0.0660,  0.1500, -1.6828],\n",
       "           [-0.4853, -0.5288,  0.7024,  ..., -0.6845, -0.3255, -1.7857],\n",
       "           [-0.4293, -0.5291,  0.6655,  ..., -0.6794, -0.4021, -1.8393],\n",
       "           [-0.4471, -0.5703,  0.7091,  ..., -0.6745, -0.3409, -1.8157],\n",
       "           [ 0.4435, -0.3123, -0.5706,  ...,  0.4295, -1.1407,  0.1324]]]),\n",
       "  tensor([[[ 0.2762, -0.2648, -0.2850,  ...,  0.2369, -1.3596,  0.6955],\n",
       "           [-0.8301,  0.2763,  0.5264,  ..., -0.0055,  0.0973, -1.1502],\n",
       "           [-0.3329, -0.1948,  0.3135,  ..., -0.7415, -0.4225, -1.2723],\n",
       "           [-0.2743, -0.2067,  0.2801,  ..., -0.7459, -0.5028, -1.3223],\n",
       "           [-0.2934, -0.2529,  0.3223,  ..., -0.7460, -0.4403, -1.3008],\n",
       "           [ 0.3594, -0.2805, -0.2679,  ...,  0.3599, -1.3199,  0.6749]]]),\n",
       "  tensor([[[ 0.0528, -0.5685, -0.4149,  ...,  0.4649, -1.0811,  1.3182],\n",
       "           [-1.0626,  0.2715,  0.2564,  ..., -0.1374,  0.4566, -0.0628],\n",
       "           [-0.5771, -0.1911,  0.0219,  ..., -0.7580,  0.0209, -0.1698],\n",
       "           [-0.5247, -0.2149, -0.0137,  ..., -0.7628, -0.0481, -0.2170],\n",
       "           [-0.5390, -0.2480,  0.0220,  ..., -0.7655,  0.0114, -0.1840],\n",
       "           [ 0.0799, -0.5603, -0.3845,  ...,  0.5632, -1.0709,  1.2951]]])))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97b84249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a6e32e9923be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# outputs.hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69739816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastlayer = model.base_model.encoder.layer[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db9b8e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastlayer = model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3922c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(lastlayer.state_dict(), \"last_layer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c0d0e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(4101, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802dd647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
